# Default values for observability-stack
# This is a YAML-formatted file.

# -- Global settings
global:
  # -- Default storage class for PVCs
  storageClass: ""
  # -- Image pull secrets
  imagePullSecrets: []

# -- Prometheus stack configuration
prometheus:
  enabled: true

# -- OpenTelemetry collector configuration  
opentelemetry:
  enabled: true

# -- Loki log aggregation
loki:
  enabled: false

# -- Custom SLO definitions
slos: []
  # - name: api-availability
  #   target: 99.9
  #   service: api-gateway
  #   metric: http_requests_total
  #   errorMetric: http_requests_total{status=~"5.."}

# -- kube-prometheus-stack sub-chart values
kube-prometheus-stack:
  # Grafana configuration
  grafana:
    enabled: true
    adminPassword: "" # Set via --set or external secret
    
    persistence:
      enabled: true
      size: 10Gi
      
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: 'custom'
            orgId: 1
            folder: 'Custom'
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/custom
              
    dashboardsConfigMaps:
      custom: observability-dashboards
      
    sidecar:
      dashboards:
        enabled: true
        searchNamespace: ALL
        
    grafana.ini:
      server:
        root_url: "%(protocol)s://%(domain)s/"
      auth.anonymous:
        enabled: false
      security:
        admin_user: admin
        
  # Prometheus configuration
  prometheus:
    prometheusSpec:
      retention: 15d
      retentionSize: "45GB"
      
      resources:
        requests:
          cpu: 500m
          memory: 2Gi
        limits:
          cpu: 2000m
          memory: 8Gi
          
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 50Gi
                
      # Service discovery
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false
      
      # Enable remote write for long-term storage (optional)
      # remoteWrite:
      #   - url: http://thanos-receive:19291/api/v1/receive
      
  # AlertManager configuration
  alertmanager:
    enabled: true
    
    alertmanagerSpec:
      storage:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 5Gi
                
    config:
      global:
        resolve_timeout: 5m
        
      route:
        group_by: ['alertname', 'namespace', 'severity']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 4h
        receiver: 'default'
        routes:
          # Critical alerts go to PagerDuty
          - match:
              severity: critical
            receiver: 'pagerduty-critical'
            continue: true
            
          # Warning alerts go to Slack
          - match:
              severity: warning
            receiver: 'slack-warnings'
            
          # Info alerts go to low-priority channel
          - match:
              severity: info
            receiver: 'slack-info'
            
      receivers:
        - name: 'default'
          # Default receiver - configure as needed
          
        - name: 'pagerduty-critical'
          pagerduty_configs:
            - service_key: '' # Set via secret
              description: '{{ template "pagerduty.default.description" . }}'
              
        - name: 'slack-warnings'
          slack_configs:
            - api_url: '' # Set via secret
              channel: '#alerts'
              title: '{{ template "slack.default.title" . }}'
              text: '{{ template "slack.default.text" . }}'
              send_resolved: true
              
        - name: 'slack-info'
          slack_configs:
            - api_url: '' # Set via secret
              channel: '#alerts-low-priority'
              send_resolved: true
              
      inhibit_rules:
        # Don't alert on warnings if there's already a critical
        - source_match:
            severity: 'critical'
          target_match:
            severity: 'warning'
          equal: ['alertname', 'namespace']
          
  # Node exporter for host metrics
  nodeExporter:
    enabled: true
    
  # Kube-state-metrics for Kubernetes object metrics
  kubeStateMetrics:
    enabled: true
    
  # Default PrometheusRules
  defaultRules:
    create: true
    rules:
      alertmanager: true
      etcd: true
      configReloaders: true
      general: true
      k8s: true
      kubeApiserverAvailability: true
      kubeApiserverBurnrate: true
      kubeApiserverHistogram: true
      kubeApiserverSlos: true
      kubeControllerManager: true
      kubelet: true
      kubeProxy: true
      kubePrometheusGeneral: true
      kubePrometheusNodeRecording: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      kubeSchedulerAlerting: true
      kubeSchedulerRecording: true
      kubeStateMetrics: true
      network: true
      node: true
      nodeExporterAlerting: true
      nodeExporterRecording: true
      prometheus: true
      prometheusOperator: true

# -- OpenTelemetry Collector sub-chart values
opentelemetry-collector:
  mode: deployment
  
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      prometheus:
        config:
          scrape_configs:
            - job_name: 'otel-collector'
              scrape_interval: 10s
              static_configs:
                - targets: ['localhost:8888']
                
    processors:
      batch:
        timeout: 10s
        send_batch_size: 1024
      memory_limiter:
        check_interval: 1s
        limit_mib: 1000
        spike_limit_mib: 200
        
    exporters:
      prometheus:
        endpoint: "0.0.0.0:8889"
        namespace: otel
      logging:
        loglevel: info
        
    service:
      pipelines:
        metrics:
          receivers: [otlp, prometheus]
          processors: [memory_limiter, batch]
          exporters: [prometheus, logging]
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [logging]
          
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

# -- Loki sub-chart values (when enabled)
loki-stack:
  loki:
    enabled: true
    persistence:
      enabled: true
      size: 50Gi
  promtail:
    enabled: true
